{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 101\n",
    "---- \n",
    "\n",
    "Este notebook é um material de apoio ao Webinar sobre Deep Learning e Tensorflow que foi ao ar no dia 31/08\n",
    "\n",
    "O objetivo desse Notebook é demonstrar os aspectos básicos acerca do Tensorflow sem esgotar suas funcionalidades e capacidades.\n",
    "\n",
    "Antes de começar, certifique-se de que está com todas as dependências instaladas, executando:\n",
    "\n",
    "    pip install -r requirements.txt\n",
    "\n",
    "As células que aparecem aqui são as mesma demonstradas no webinar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro importamos o Tensorflow e as outras bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos criar algumas variáveis e performar uma simples conta matemática. Vale lembrar que o Tensorflow primeiro cria um grafo computacional com todas as variáveis, as inicializa e depois as computa.\n",
    "\n",
    "Nessa primeira etapa vamos apenas criar um grafo com as seguintes variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criamos, em seguida, um inicializador de variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora temos que iniciar uma seção do Tensorflow para processar nosso resultado|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "42!\n",
    "\n",
    "Acabamos de calcular a resposta da vida, universo e tudo mais utilizando o Tensorflow!!!\n",
    "\n",
    "Pode não parecer algo realmente importante, mas na verdade o Tensorflow fez esse cálculo utilizando um nível muito mais baixo do que o python consegue utilizar e isso é o que faz dele um framework especial.\n",
    "\n",
    "Agora sabemos que o Tensorflow está funcionando, vamos fazer uma pequena rede neural para identificar dígitos escritos à mão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning com Tensorflow\n",
    "---\n",
    "\n",
    "Para esse modelo vamos usar imagens de dígitos escritos a mão, cada dígito foi convertido para uma imagem de 28X28 com pixels de 0 até 255. Primeiro extraímos os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos vizualizar a imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_xs, batch_ys = mnist.test.next_batch(1)\n",
    "two_d = (np.reshape(batch_xs, (28, 28)) * 255).astype(np.uint8)\n",
    "plt.imshow(two_d, 'gray',interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "print \"Número da imagem: \" + str(batch_ys[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o Tensorflow primeiro constrói um grafo para depois calcular todas as variáveis, primeiro temos que construir a estrutura da nossa rede neural.\n",
    "\n",
    "Como estamos lidando com imagens, podemos pensar que cada pixel é um input que vai passar por um neurônio, entretanto se essa estivéssemos construíndo uma rede convolucional essa ideia seria um pouco diferente.\n",
    "\n",
    "A rede que criaremos a seguir é uma Dense Neural Netowork. Nesta rede existe um neurônio para cada input e todos os neurônios de uma camada estão ligados aos neurônios da próxima camada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos primeiro definir qual será a função de construção das camadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        init=tf.truncated_normal((n_inputs,n_neurons), stddev=2/np.sqrt(n_inputs))\n",
    "        w = tf.Variable(init, name=\"weights\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name = \"biases\")\n",
    "        z = tf.matmul(X, w) + b\n",
    "        if activation:\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código acima está apenas construindo a estrutura de uma camada densa de neurônios.\n",
    "A função recebe os inputs X, a quantidade de neurônios presentes na camada, o nome da camada e se tem ou não um ativação ao final de cada neurônio\n",
    "\n",
    "Em seguida, define o escopo (\"nome da camada\") no grafo do Tensorflow com o nome escolhido.\n",
    "O número de inputs é deduzido a partir dos próprios inputs.\n",
    "Aos pesos (weights) são atribuídos valores aleatórios normalmente.\n",
    "Aos viés são atribuídos o valor 0.\n",
    "\n",
    "Vale lembrar que a função de um neurônio é dada por Y = f((w*x)+b). Desta equação já definimos o 'w' e o 'b', agora temos que definir a função de ativação.\n",
    "\n",
    "Nesse caso a função de ativação será a função ReLU (rectified linear unit), essa função é dada por z = max(0,x). Ou seja retorna zero sempre que x<0 ou x se x>0.\n",
    "\n",
    "Para definir essa função utilizamos o método nativo do Tensorflow tf.nn.relu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construindo a rede neural\n",
    "----\n",
    "\n",
    "Agora que definimos nossa função de camadas, vamos construir nossa rede neural.\n",
    "\n",
    "Primeiro temos que dizer para o Tensorflow qual é o tipo do dado de entrada e saída. Para isso, utilizamos o método placeholder e passamos o tipo do dado, a forma do dado e o nome do placeholder. Note que os dados de entrada têm uma forma (None, 28*28), no qual o primeiro valor se refere ao tamanho do batch de treinamento/predição e o segundo valor é o tamanho de cada imagem (nesse caso, nossas imagens têm tamanho 28X28). Vale lembrar que as imagens serão transformadas de uma matriz 28X28 para um array de tamanho 784.\n",
    "\n",
    "Em seguida criamos a rede neural em si. Essa rede simples performa muito bem nesse dataset, e é composta de 3 camadas com com a seguinte composição:\n",
    "    - layer 1: 300 neurônios\n",
    "    - layer 2: 100 neurônios\n",
    "    - layer 3: 10 neurônios - camada de saída\n",
    "    \n",
    "A saída desta rede neural é chamada de logits e é um array de 10 valores, cada um representando um neurônio. Isso por que em vez da rede responder um único dígito de 0-9, irá retornar um array com a ativação de cada um dos neurônios. Ou seja, podemos dizer que cada neurônio está responsável para identificar um único dígito. Posteriormente, iremos utilizar uma função chamada de softmax para normalizar as saídas em valores entre 0. e 1. que representam a própria probabilidade de confiança de cada neurônio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define os placeholders de entrada e saída\n",
    "X=tf.placeholder(tf.float32,(None,28*28),name=\"X\")\n",
    "y=tf.placeholder(tf.int64,shape=(None),name=\"y\")\n",
    "\n",
    "# Cria a rede utilizando a função definida anteriormente\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    layer1= dense_layer(X, 300, \"hidden1\", activation=True)\n",
    "    layer2= dense_layer(layer1, 100, \"hidden2\", activation=True)\n",
    "    logits= dense_layer(layer2, 10, \"logits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o processo de treinamento poder acontecer precisamos definir um parâmetro de erro/perda a ser minimizado pelo gradient descent. Nesse caso, vamos utilizar uma função de cross-entropia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a os parâmetros de perda\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss=tf.reduce_mean(xentropy,name=\"loss\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora parametrizamos o valor da taxa de aprendizado, que dita a quantidade que o gradiente irá variar em cada interação. Uma taxa muito grande pode fazer com que a rede passe do ponto e não consiga convergir para o ponto ótimo. Por outro lado, uma taxa muito pequena vai impedir que o aprendizado aconteça\n",
    "\n",
    "O Tensorflow toma conta de fazer todo o Back Propagation automaticamente! Para isso precisamos criar um operador para o otimizador e passar qual valor deve ser minimizado. É por meio desse operador que o Tensorflow irá atualizar todos os pesos e viés da rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um otimizador e parametriza um operador para esse otimizador\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op=optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A última coisa antes de começar a treinar a rede é definir algumas métricas para acompanhar o processo de treinamento. A métrica que vamos definir a seguir é a acuracidade: o % de acerto de um batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria as variáveis de avaliação\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct= tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    conf = tf.reduce_max(tf.nn.softmax(logits, axis=1), axis=1)\n",
    "\n",
    "    tf.summary.scalar(\"Accuracy\",accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando o Modelo\n",
    "---\n",
    "\n",
    "Por fim, vamos treinar o modelo. Antes vamos definir o duas coisas:\n",
    "    - Época: uma iteração pelo dataset de treino\n",
    "    - Batch: um subset do dataset de treino\n",
    "    \n",
    "O código abaixo seleciona um batch por vez e o fornece para o operador de treino com:\n",
    "\n",
    "    for iteration in range(mnist.train.num_examples//batch_size):\n",
    "        X_batch, y_batch=mnist.train.next_batch(batch_size)\n",
    "        sess.run(training_op,feed_dict={X:X_batch,y:y_batch})\n",
    "\n",
    "O operador de treino tem por objetivo realizar o gradient descent com o intuito de minimizar o erro final do modelo. Isso acontece por meio do ajuste de todos os pesos e viés da rede, na realidade a matemática por trás disso é um tanto complexa e não cabe explicá-la aqui.\n",
    "\n",
    "Toda a vez que o dataset de treino é lido por inteiro, o dataset de test é utilizado para avaliar o modelo.\n",
    "\n",
    "Com apenas 40 épocas esse modelo consegue atingir um resultado de 97% de acuracidade!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Cria os arquivos do tensorboard\n",
    "    now = datetime.utcnow()\n",
    "    train_writer = tf.summary.FileWriter('logdir/train-{}'.format(now),sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('logdir/test-{}'.format(now))\n",
    "    \n",
    "    # Inicia as variáveis do tensorflow\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "    \n",
    "        # Itera n baches do dataset\n",
    "        for iteration in range(mnist.train.num_examples//batch_size):\n",
    "            X_batch, y_batch=mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op,feed_dict={X:X_batch,y:y_batch})\n",
    "            \n",
    "        # Adiciona as métricas para o Tensorboar\n",
    "        train_writer.add_summary(merged.eval(feed_dict={X:X_batch,y:y_batch}),epoch)\n",
    "        test_writer.add_summary(merged.eval(feed_dict={X:mnist.test.images,y:mnist.test.labels}),epoch)\n",
    "\n",
    "        # Calcula métricas de acurácia para acompanhar o treinamento\n",
    "        acc_train=accuracy.eval(feed_dict={X:X_batch,y:y_batch})\n",
    "        acc_test=accuracy.eval(feed_dict={X:mnist.test.images,y:mnist.test.labels})\n",
    "        print(epoch,\"Train accuracy:\",acc_train,\"Test accuracy:\",acc_test)\n",
    "        \n",
    "        # Salva os checkpoits por cada Época\n",
    "        save_path=saver.save(sess,\"./logdir/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando o Modelo\n",
    "---\n",
    "\n",
    "Por fim, utilizamos o mesmo modelo para classificar uma imagem do dataset de treino.\n",
    "\n",
    "Nesse caso o output do modelo são logits, onde o index do maior argumento representa o número classificado. Se uma função softmax for utilizada nesses logits tem-se a probabilidade da classificação "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_xs, batch_ys = mnist.test.next_batch(1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./logdir/my_model_final.ckpt\")\n",
    "    X_new_scaled=batch_xs\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled})\n",
    "    y_pred=np.argmax(Z, axis=1)\n",
    "    confidence = sess.run(tf.nn.softmax(Z, axis=1))\n",
    "\n",
    "print \"Prediction: {} | Confidence: {}%\".format(y_pred[0],confidence[0][y_pred][0] * 100)\n",
    "    \n",
    "two_d = (np.reshape(batch_xs, (28, 28)) * 255).astype(np.uint8)\n",
    "plt.imshow(two_d, 'gray',interpolation='nearest')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
